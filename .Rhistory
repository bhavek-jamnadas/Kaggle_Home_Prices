# Let's start scraping the main head and paragraph text/comments of a single Reddit page
#install.packages("rvest")
library(rvest)
reddit_wbpg <- read_html("https://www.reddit.com/r/politics/comments/a1j9xs/partisan_election_officials_are_inherently_unfair/")
reddit_wbpg %>%
html_node("title") %>%
html_text()
reddit_wbpg %>%
html_nodes("p.s90z9tc-10") %>%
html_text()
reddit_wbpg <- read_html("https://www.reddit.com/r/politics/new")
reddit_wbpg %>%
html_node("title") %>%
html_text()
reddit_wbpg %>%
html_nodes("p.s90z9tc-10") %>%
html_text()
comments/a1j9xs/partisan_election_officials_are_inherently_unfair/
reddit_wbpg <- read_html("https://www.reddit.com/r/politics/comments/a1j9xs/partisan_election_officials_are_inherently_unfair/")
reddit_wbpg <- read_html("https://www.reddit.com/r/politics/comments/a1j9xs/partisan_election_officials_are_inherently_unfair/")
reddit_wbpg %>%
html_nodes("p.s90z9tc-10") %>%
html_text()
reddit_wbpg %>%
html_nodes("p.s1w8oh2o-10 bQeEFC") %>%
html_text()
reddit_wbpg %>%
html_nodes("p.s1w8oh2o-10") %>%
html_text()
# Let's scrape the time and URL of all latest pages published on Reddit's r/politics
reddit_political_news <- read_html("https://www.reddit.com/r/politics/new/")
time <- reddit_political_news %>%
html_nodes("a._3jOxDPIQ0KaOWpzvSQo-1s") %>%
html_text()
time
urls <- reddit_political_news %>%
html_nodes("a._3jOxDPIQ0KaOWpzvSQo-1s") %>%
html_attr("href")
urls
time
time <- realestate %>%
html_nodes("a.branding branding--large") %>%
html_text()
realestate <- read_html("https://www.reddit.com/r/politics/comments/a1j9xs/partisan_election_officials_are_inherently_unfair/")
address <- realestate %>%
html_nodes("a.branding branding--large") %>%
html_text()
address
realestate %>%
html_nodes("residential-card__secondary-text rui-truncate") %>%
html_text()
realestate %>%
html_nodes("a.residential-card__secondary-text rui-truncate") %>%
html_text()
realestate %>%
html_nodes("p.residential-card__secondary-text rui-truncate") %>%
html_text()
realestate %>%
html_nodes("p.residential-card__secondary-text rui-truncate")
library(h2o)
h2o.init()
# import the covtype dataset:
# This dataset is used to classify the correct forest cover type
# original dataset can be found at https://archive.ics.uci.edu/ml/datasets/Covertype
covtype <- h2o.importFile("https://s3.amazonaws.com/h2o-public-test-data/smalldata/covtype/covtype.20k.data")
# convert response column to a factor
covtype[,55] <- as.factor(covtype[,55])
covtype[,55]
# set the predictor names and the response column name
predictors <- colnames(covtype[1:54])
response <- 'C55'
# split into train and validation sets
covtype.splits <- h2o.splitFrame(data =  covtype, ratios = .8, seed = 1234)
train <- covtype.splits[[1]]
valid <- covtype.splits[[2]]
# look at the frequencies of each class
print(h2o.table(covtype['C55']))
pacman::p_load('dplyr','corrplot','sparklyr','ggplot2','scales','lubridate','data.table'
,'dineq','MLmetrics','rsparkling','pandas')
setwd('C:/Users/bhave/OneDrive/Documents/Coding/Kaggle/House Price')
# h2o.init()
df<-fread('data/train.csv')
pacman::p_load('dplyr','corrplot','sparklyr','ggplot2','scales','lubridate','data.table'
,'dineq','MLmetrics','rsparkling','h2o')
# h2o.init()
df<-fread('data/train.csv')
require(lattice)
require(ggplot2)
pairs(df)
ggcorplot(df)
install.packages('reticulate')
#loading required R libraries
library(reticulate) #the superpower bridges python and R
library(reticulate)
use_python('C:/Users/bhave/AppData/Local/Programs/Python/Python36-32/python.exe')
sns <- import('seaborn')
plt <- import('matplotlib.pyplot')
sns <- import('seaborn')
use_python('C:/Users/bhave/AppData/Local/Programs/Python/Python36-32/python.exe')
#importing required Python libraries/modules
sns <- import('seaborn')
plt <- import('matplotlib.pyplot')
pd <- import('pandas')
py_install("numpy", pip = TRUE)
py_install("seaborn", pip = TRUE)
#importing required Python libraries/modules
sns <- import('seaborn')
plt <- import('matplotlib.pyplot')
pd <- import('pandas')
sns$pairplot(r_to_py(df))
gc()
gc()
pacman::p_load('dplyr','corrplot','sparklyr','ggplot2','scales','lubridate','data.table'
,'dineq','MLmetrics','rsparkling','h2o')
setwd('C:/Users/bhave/OneDrive/Documents/Coding/Kaggle/House Price')
# h2o.init()
df<-fread('data/train.csv')
library(reticulate)
use_python('C:/Users/bhave/AppData/Local/Programs/Python/Python36-32/python.exe')
#importing required Python libraries/modules
sns <- import('seaborn')
plt <- import('matplotlib.pyplot')
pd <- import('pandas')
colnames(df)
df1 <- df[,c('SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt')]
sns$pairplot(r_to_py(df1))
#display the plot
plt$show()
rm(df1)
